{"_id":"57f6ca6b-62f8-3adb-9901-6683289646f6","authorName":"Chris Pratt","authorEmailEncrypted":"C9wDh//nczs4WyNcT+B6QGGrAFDa5Ffb5sF6T4q/L8nAfpltmOBxP2dDipB3pcAx+Wy7ch2Ce6PgSVx10BR2vM67fnBKFNO57nrVPwc33/aUKEkJAkj2rR12Nyw9vcZb5yvYYzq0HcA4gqZXl4xxtHWztP8/0VeAwSov13146lAqv0uC9Cl0UbhVQoQ1sPjbVTCxyh9a+OU6Ni0PPYF9sfu45M6LMFVayT8G+J5k2p9nBZxEHFR7Dk9CelMO52urSzXyjT0HnDtfrSEhsO6qw10edUGsE/7stoEwUyEnIoVqqkEw/XnUDHcvaJ3huBc9AV6OAdibF03BzdCurzCYZDmj2UcMjFnI1RzaaPIso6T7Mhp0l33Lxkxa+J9aM4L+co2v0sNHvdXwSA4z47VQ1ORxQBE22hEFOd9NLsp5oPXEXzUEamP+mPAu+DgnWOcdfF2y3GTDYk81qJsOvzNen3iGh88OpN3Q8PTgHYotsUlA+QH1AsRtKaeBKdTqX/M9PsSX63lox0U6wGG2LCtBMZFj0ocQwvfLN7ujA2OYgdz3HqweBMqYuFcwxOwX3tbj9tC/Xe6ukabQy87Hy3E703tbEsxeEYBO9nujAJZWrjYDcXDhaJ3VhQOAG5UNxHWHptKHo92wsBPA7LG6uZefNorpipusY61Og0o8OIBTOoY=","authorEmailMD5":"9d0a45dd92ecc2cb15cdbc5f80befb15","authorUri":"","authorUserId":"disqus:chrisdpratt","message":"There's an absolute ton of moving parts involved in streaming a file. Having the content length allows some optimizations to be made that cannot occur without it. For example, if you need to reserve space in RAM or on disk, you can do it if you know the content length. Otherwise, you simply have to make a best guess and then potentially move and reshuffle things if the content ends up exceeding that. This is similar to `MemoryStream()` vs. `MemoryStream(N)`, where the first needs to be constantly resized, while the latter remains constant (at least as long as you don't exceed N).\r\n\r\nHowever, as Stephen pointed out, you can only get the content length if you create the zip in memory, in which case, you're no longer streaming. The speed you lose by not having the optimizations that come along with a set content length is a trade-off for less RAM utilization and a more immediate start to the download process.\r\n\r\nIf you read through the entire issue, though, you'll see that while setting the content-length did provide faster speeds, by far the biggest boost came from setting acceptably high buffers. The reason is two-fold. First most disks are more efficient at large block writes than small. If you've ever attempted to copy a single 2GB file versus 2GB of many small files, you've seen this first hand. Second, having a larger buffer means that the same optimizations with a known content-length can at least be made as well for the length of that buffer. While data still needs to be shuffled around, it doesn't have to happen quite as often, leading to faster overall through put.\r\n\r\nAll that said, your primary limiting factors are always going to be mostly the read speed of the source medium, the write speed of the destination medium, and the bandwidth of the connection between the two. Those factors will usually outplay anything going on with `Content-Length` being set or not.","postId":"2016_11_streaming-zip-on-aspnet-core-66a49df2-4f92-385b-8999-87a9f3054346","replyTo":"46b49d28-4ca9-32d3-a56f-7403d899b9c7","date":"2018-02-01T18:29:29Z","timestamp":1517509769}