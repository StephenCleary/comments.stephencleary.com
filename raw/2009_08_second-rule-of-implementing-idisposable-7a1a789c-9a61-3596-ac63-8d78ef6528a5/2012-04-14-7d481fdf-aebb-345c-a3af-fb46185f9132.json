{"_id":"7d481fdf-aebb-345c-a3af-fb46185f9132","authorName":"Stephen Cleary","authorEmailEncrypted":"","authorEmailMD5":"3db7b6e14d9da42751e4bab03bc4d034","authorUri":"https://stephencleary.com/","authorUserId":"","authorFallbackAvatar":"","message":"I would say it depends on whether there is an actual \"leak\" or not. What happens if Disconnect is never called? If the system as a whole recovers from that situation (e.g., the external system will eventually detect the connection is no longer there or just times it out, and if the external system allows other connections from the same source) then it can be treated as a \"managed\" resource - it's inefficient but there's no leak.  \r\n\r\nThat's probably the best way to go, because the system as a whole must be able to recover from connection problems. The only other situation I can think of is if the (client-side) API layer itself has some restriction, like only allowing one connection per process. In that case, I would lean towards putting in a finalizer which actually (intentionally) crashes.","postId":"2009_08_second-rule-of-implementing-idisposable-7a1a789c-9a61-3596-ac63-8d78ef6528a5","replyTo":"","date":"2012-04-14T02:12:19Z","timestamp":1334369539}